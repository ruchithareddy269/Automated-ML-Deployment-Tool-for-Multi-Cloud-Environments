{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4dfa2d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe45be8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from talk_openai import MyOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "import os\n",
    "from utils import *\n",
    "import re\n",
    "from langchain_core.tools import tool\n",
    "from langchain.agents import initialize_agent, AgentType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d79eb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54b2a709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Python 3.10.12'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.check_output(['python', '--version']).decode('utf-8').strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57a135b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Python 3.10.12'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.check_output([sys.executable, '--version']).decode('utf-8').strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e92085b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_script_path = \"iris_model_inference/inference.py\"\n",
    "MODEL_NAME = \"gpt-4o-mini\"\n",
    "# Create a ChatOpenAI object \n",
    "inference_script_generator_llm = MyOpenAI(model=MODEL_NAME)\n",
    "inference_script_validator_llm = MyOpenAI(model=\"gpt-4o-mini\")\n",
    "inference_script_dir = \"/Users/neel/Developer/sagemaker_test/inference\"\n",
    "terraform_dir = \"/Users/neel/Developer/sagemaker_test/terraform\"\n",
    "model_type = \"MultiClass Classification\"\n",
    "data_shape = \"(,4)\"\n",
    "python_version = get_python_version()\n",
    "packages = get_installed_packages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cb7317c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_script_dir=\"/Users/neel/Developer/sagemaker_test/inference\"\n",
    "terraform_dir = \"/Users/neel/Developer/sagemaker_test/terraform\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a863c52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## generaring inference script plus other artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8da6b1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sagemaker_inference_bundle(training_script, model_type, data_shape, feedback=None):\n",
    "    \"\"\"Use an LLM to generate inference.py + serve for a SageMaker endpoint.\"\"\"\n",
    "    if feedback is None:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert Python developer.\"},\n",
    "            {\"role\": \"user\", \"content\": f'''\n",
    "        You are to generate *two* files for AWS SageMaker custom inference:\n",
    "\n",
    "        1) **inference.py**  \n",
    "        - From the training script identity the name of the model file and the format in which the model is saved. And use similar name and format for loading the model\n",
    "        - Use FastAPI, listen on 0.0.0.0:8080  \n",
    "        - **GET /ping** â†’ return JSON {{ \"status\": \"OK\" }} with HTTP status code **200**  \n",
    "        - **POST /invocations** â†’ accept `application/json` with key `\"instances\"`,  \n",
    "            preprocess into the shape {data_shape},  \n",
    "            load the trained model from `/opt/ml/model`,  \n",
    "            call `model.predict(...)`,  \n",
    "            and return `{{\"predictions\": [...]}}`.  \n",
    "        - Follow best practices: exception handling, logging, and specify `status_code=200` on ping.\n",
    "        - The model will be saved is `/opt/ml/model/`, you will understand how the model is saved from the training script.\n",
    "\n",
    "        2) **serve**  \n",
    "        - A bashâ€executable script that SageMaker will invoke to launch your FastAPI app  \n",
    "        - e.g. `uvicorn inference:app --host 0.0.0.0 --port 8080`\n",
    "\n",
    "        ### Context:\n",
    "        The training script  used is:\n",
    "        \\\"\\\"\\\"\n",
    "        {training_script}\n",
    "        \\\"\\\"\\\"\n",
    "        Model type: {model_type}.\n",
    "\n",
    "        Provide the *complete* contents of both files (with proper shebang for `serve`).\n",
    "        ''' }\n",
    "                ]\n",
    "    else:\n",
    "        messages = feedback\n",
    "    response = inference_script_generator_llm.invoke(messages)\n",
    "    return response\n",
    "    \n",
    "def evaluate_inference_script_with_llm(inference_script):\n",
    "    \"\"\"Use an LLM to evaluate the generated inference script and provide feedback.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a code reviewer and expert in Python API development.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"\"\"\n",
    "        You are reviewing an inference script generated for exposing a trained ML model as an API on AWS SageMaker.\n",
    "\n",
    "        ### Task:\n",
    "        - Analyze the script for correctness, best practices, and completeness. (it should have a /ping and /invocations endpoint)\n",
    "        - Identify any missing components or improvements.\n",
    "        - should load the model from /opt/ml/model/ and call model.predict(...)\n",
    "        - If the script is perfect, respond with 'No changes needed.'\n",
    "        - If changes are required, specify what needs to be improved.\n",
    "        - there should be a serve script that is executable and launches the FastAPI app.\n",
    "\n",
    "        ### Script to review:\n",
    "        ```\n",
    "        {inference_script}\n",
    "        ```\n",
    "\n",
    "        ### Expected Output:\n",
    "        - Either 'No changes needed.' OR a detailed improvement plan.\n",
    "        \"\"\"}\n",
    "    ]\n",
    "    \n",
    "    return inference_script_validator_llm.invoke(messages)\n",
    "\n",
    "def actor_critic_inference_script(training_script, model_type, data_shape):\n",
    "        \"\"\"\n",
    "        Runs the actor-critic loop:\n",
    "        - Actor generates an inference script.\n",
    "        - Critic evaluates the script.\n",
    "        - If the script is sufficient, exit early.\n",
    "        - Otherwise, Actor refines the script based on feedback.\n",
    "        \"\"\"\n",
    "        feedback = None  # No feedback initially\n",
    "\n",
    "        for _ in range(2):  # Max iterations: 1\n",
    "            # Step 1: Actor generates inference script\n",
    "            inference_script = generate_sagemaker_inference_bundle(training_script, model_type, data_shape, feedback)\n",
    "\n",
    "            # Step 2: Critic evaluates the script\n",
    "            critic_feedback = evaluate_inference_script_with_llm(inference_script)\n",
    "\n",
    "            if \"no changes needed\" in critic_feedback.lower():\n",
    "                print(\"âœ… Inference script is satisfactory. Exiting early.\")\n",
    "                return inference_script  # Early exit if script is sufficient\n",
    "\n",
    "            # Step 3: Actor refines script using critic's feedback\n",
    "            feedback = [\n",
    "                {\"role\": \"assistant\", \"content\": inference_script},\n",
    "                {\"role\": \"user\", \"content\": f\"Revise based on this feedback:\\n{critic_feedback}\"}\n",
    "            ]\n",
    "\n",
    "        print(\"ðŸ”„ Max iterations reached. Returning final script.\")\n",
    "        return inference_script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "db8cba24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Max iterations reached. Returning final script.\n"
     ]
    }
   ],
   "source": [
    "response = actor_critic_inference_script(training_script_path, model_type, data_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0030000d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hereâ€™s the revised version of the `inference.py` and `serve` script based on your feedback:\\n\\n### 1) `inference.py`\\n\\n```python\\nimport os\\nimport json\\nimport logging\\nimport numpy as np\\nimport joblib  # Corrected import for joblib\\nfrom fastapi import FastAPI, HTTPException\\nfrom pydantic import BaseModel\\n\\n# Set up logging\\nlogging.basicConfig(level=logging.INFO)\\nlogger = logging.getLogger(__name__)\\n\\n# Create FastAPI app\\napp = FastAPI()\\n\\n# Define the request model\\nclass PredictionRequest(BaseModel):\\n    instances: list\\n\\n# Load the model\\nmodel_path = os.getenv(\\'MODEL_PATH\\', \\'/opt/ml/model/model.joblib\\')  # Make model path configurable\\ntry:\\n    model = joblib.load(model_path)\\n    logger.info(\"Model loaded successfully.\")\\nexcept Exception as e:\\n    logger.error(f\"Failed to load model: {e}\")\\n    raise\\n\\n@app.get(\"/ping\", status_code=200)\\nasync def ping():\\n    \"\"\"Health check endpoint.\"\"\"\\n    return {\"status\": \"OK\"}\\n\\n@app.post(\"/invocations\")\\nasync def invoke(request: PredictionRequest):\\n    \"\"\"Endpoint for making predictions.\\n\\n    Args:\\n        request (PredictionRequest): The request containing instances for prediction.\\n\\n    Returns:\\n        dict: A dictionary containing predictions.\\n\\n    Raises:\\n        HTTPException: If input validation fails or prediction fails.\\n    \"\"\"\\n    try:\\n        # Validate input\\n        instances = request.instances\\n        if not isinstance(instances, list) or len(instances) == 0:\\n            raise ValueError(\"Input must be a non-empty list.\")\\n        \\n        # Validate input shape dynamically\\n        input_shape = len(instances[0]) if isinstance(instances[0], list) else 0\\n        expected_shape = model.n_features_in_  # Get expected input shape from the model\\n        if input_shape != expected_shape:\\n            raise ValueError(f\"Input must have {expected_shape} features, but got {input_shape}.\")\\n\\n        # Convert to numpy array\\n        input_data = np.array(instances)\\n        logger.info(f\"Input data shape: {input_data.shape}\")\\n\\n        # Make predictions\\n        predictions = model.predict(input_data).tolist()  # Convert to list for JSON serialization\\n        return {\"predictions\": predictions}\\n    \\n    except ValueError as ve:\\n        logger.error(f\"Input validation error: {ve}\")\\n        raise HTTPException(status_code=400, detail=str(ve))\\n    except Exception as e:\\n        logger.error(f\"Error during prediction: {e}\")\\n        raise HTTPException(status_code=500, detail=\"An error occurred during prediction.\")\\n```\\n\\n### 2) `serve`\\n\\n```bash\\n#!/usr/bin/env bash\\n# This script will be executed by SageMaker to launch the FastAPI app\\n\\n# Check if uvicorn is installed\\nif ! command -v uvicorn &> /dev/null; then\\n    echo \"uvicorn could not be found. Please install it.\"\\n    exit 1\\nfi\\n\\n# Start the FastAPI app\\nuvicorn inference:app --host 0.0.0.0 --port 8080\\n```\\n\\n### 3) `requirements.txt`\\n\\nMake sure to include a `requirements.txt` file with the following dependencies:\\n\\n```\\nfastapi\\nuvicorn\\njoblib\\nscikit-learn\\nnumpy\\n```\\n\\n### Summary of Changes:\\n- **Model Loading Import**: Corrected the import statement for `joblib`.\\n- **Input Data Reshaping**: Added dynamic validation for the input shape based on the model\\'s expected input.\\n- **Error Handling**: Improved error handling to differentiate between validation errors and prediction errors.\\n- **Logging**: Enhanced logging messages for better context.\\n- **Model Path Configuration**: Made the model path configurable via an environment variable.\\n- **Serve Script**: Added a shebang line and a check for `uvicorn` installation.\\n- **Dependency Management**: Included a `requirements.txt` file for easy installation of dependencies.\\n- **Documentation**: Added docstrings to functions for better readability and maintainability.\\n\\nThese changes should make the script more robust, maintainable, and user-friendly.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8fc1e5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "from langchain_core.tools import StructuredTool\n",
    "from pydantic import BaseModel\n",
    "class SaveFileInput(BaseModel):\n",
    "    file_path: str\n",
    "    content: str\n",
    "\n",
    "# Define the saving tool\n",
    "@tool(args_schema=SaveFileInput)\n",
    "\n",
    "def save_file_tool(file_path: str, content: str) -> str:\n",
    "    \"\"\"Save content to a file at the given path.\"\"\"\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(content)\n",
    "    return f\"Saved to {file_path}\"\n",
    "\n",
    "def extract_and_save_inference_files(response_str: str, save_dir: str = ''):\n",
    "        llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "        tools = [save_file_tool]\n",
    "\n",
    "        agent = initialize_agent(\n",
    "            tools=tools,\n",
    "            llm=llm,\n",
    "            agent=AgentType.OPENAI_FUNCTIONS,\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "        response = agent.invoke(\n",
    "            f\"\"\"\n",
    "            1. Extract the inference script and serve script from the response string.\n",
    "            2. For each extracted script:\n",
    "                - If it is the inference script, save it as '{inference_script_dir}/inference.py'.\n",
    "                - If it is the serve script, save it as '{inference_script_dir}/serve'.\n",
    "                - Otherwise, save it as '{inference_script_dir}/<appropriate file_name and extension>'.\n",
    "            3. Use the `save_file_tool` tool to save the content.\n",
    "            4. Do not output anything else except confirming tool usage.\n",
    "\n",
    "            ### Text:\n",
    "            {response_str}\n",
    "            \"\"\"\n",
    "        )\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "572caf65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `save_file_tool` with `{'file_path': '/Users/neel/Developer/sagemaker_test/inference/inference.py', 'content': 'import os\\nimport logging\\nimport numpy as np\\nimport joblib  # Updated import for joblib\\nfrom fastapi import FastAPI, HTTPException\\nfrom pydantic import BaseModel\\n\\n# Set up logging\\nlogging.basicConfig(level=logging.INFO)\\nlogger = logging.getLogger(__name__)\\n\\n# Define the FastAPI app\\napp = FastAPI()\\n\\n# Load the model\\nmodel_path = os.path.join(\"/opt/ml/model\", \"iris_model.pkl\")\\ntry:\\n    model = joblib.load(model_path)\\n    logger.info(\"Model loaded successfully.\")\\nexcept Exception as e:\\n    logger.error(f\"Failed to load model: {str(e)}\")\\n    raise RuntimeError(\"Model loading failed.\")\\n\\n# Define the request body for the POST /invocations endpoint\\nclass PredictionRequest(BaseModel):\\n    instances: list\\n\\n@app.get(\"/ping\", status_code=200)\\nasync def ping():\\n    \"\"\"Health check endpoint.\"\"\"\\n    logger.info(\"Ping received\")\\n    return {\"status\": \"OK\"}\\n\\n@app.post(\"/invocations\")\\nasync def predict(request: PredictionRequest):\\n    \"\"\"Predict the class of the input instances.\"\"\"\\n    try:\\n        instances = request.instances\\n        # Validate input format\\n        if not isinstance(instances, list) or not all(isinstance(i, list) and len(i) == 4 for i in instances):\\n            raise HTTPException(status_code=400, detail=\"Invalid input format. Expected a list of lists with 4 elements each.\")\\n        \\n        # Convert to numpy array\\n        input_data = np.array(instances)\\n        \\n        # Make predictions\\n        predictions = model.predict(input_data).tolist()\\n        \\n        logger.info(f\"Predictions made: {predictions} for input: {instances}\")\\n        return {\"predictions\": predictions}\\n    except ValueError as ve:\\n        logger.error(f\"Value error during prediction: {str(ve)}\")\\n        raise HTTPException(status_code=400, detail=str(ve))\\n    except Exception as e:\\n        logger.error(f\"Error during prediction: {str(e)}\")\\n        raise HTTPException(status_code=500, detail=\"Internal Server Error\")\\n'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mSaved to /Users/neel/Developer/sagemaker_test/inference/inference.py\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `save_file_tool` with `{'file_path': '/Users/neel/Developer/sagemaker_test/inference/serve', 'content': '#!/bin/bash\\n# This script will be executed to launch the FastAPI app\\n\\n# Check if the model is loaded successfully\\nif [ ! -f /opt/ml/model/iris_model.pkl ]; then\\n    echo \"Model file not found. Exiting.\"\\n    exit 1\\nfi\\n\\n# Start the FastAPI app\\nuvicorn inference:app --host 0.0.0.0 --port 8080\\n'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mSaved to /Users/neel/Developer/sagemaker_test/inference/serve\u001b[0m\u001b[32;1m\u001b[1;3mThe scripts have been saved successfully using the tool.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "raw_files = extract_and_save_inference_files(response, inference_script_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3d274a",
   "metadata": {},
   "source": [
    "## Generating the requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e28f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_imported_libraries(python_script):\n",
    "        \"\"\"\n",
    "        Extracts imported libraries from a Python script.\n",
    "\n",
    "        Args:\n",
    "            python_script (str): The Python script content.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of unique libraries found in the script.\n",
    "        \"\"\"\n",
    "        matches = re.findall(r\"^\\s*(?:import|from)\\s+([\\w\\d_\\.]+)\", python_script, re.MULTILINE)\n",
    "        return list(set(matches))  # Remove duplicates\n",
    "\n",
    "\n",
    "def get_requirements_txt():\n",
    "    inference_script_dir = \"/Users/neel/Developer/sagemaker_test/inference\"\n",
    "    inference_script_path = os.path.join(inference_script_dir, \"inference.py\")\n",
    "    inference_script = file_reader(inference_script_path)\n",
    "    libraries_str = extract_imported_libraries(inference_script)\n",
    "    messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert Python package manager.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"\"\"\n",
    "            for the inference file you generated earlier, I need to create a `requirements.txt` file.\n",
    "            ### Task:\n",
    "            - Generate a `requirements.txt` file based on the following libraries used in the script:\n",
    "             - Also include known libraries which are not in the script but are commonly used for model inference.\n",
    "            - Ensure the libraries are listed in a format suitable for `pip install`.\n",
    "            {libraries_str}\n",
    "            - Python version is {python_version}.\n",
    "            - To check which version to install refer to the\" this list of all installed packages in the environment: {packages}\n",
    "    \"\"\"}\n",
    "        ]\n",
    "\n",
    "    return inference_script_generator_llm.invoke(messages)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4137eef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_requirements_txt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4f98e5dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the libraries you provided and the common libraries used for model inference, here is a `requirements.txt` file that includes the necessary packages along with their versions:\n",
      "\n",
      "```plaintext\n",
      "fastapi==0.115.6\n",
      "joblib==1.4.2\n",
      "numpy==1.26.4\n",
      "pydantic==2.9.2\n",
      "requests==2.32.3\n",
      "scikit-learn==1.0.2\n",
      "scipy==1.10.1\n",
      "uvicorn==0.22.0\n",
      "python-dotenv==1.0.1\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "- **fastapi**: For building APIs.\n",
      "- **joblib**: For saving and loading models.\n",
      "- **numpy**: For numerical operations.\n",
      "- **pydantic**: For data validation and settings management.\n",
      "- **requests**: For making HTTP requests, often used in inference scenarios.\n",
      "- **scikit-learn**: For machine learning utilities.\n",
      "- **scipy**: For scientific computing.\n",
      "- **uvicorn**: ASGI server for running FastAPI applications.\n",
      "- **python-dotenv**: For loading environment variables from a `.env` file, useful for configuration.\n",
      "\n",
      "You can create a `requirements.txt` file with the above content and use it to install the necessary packages with `pip install -r requirements.txt`.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bfb04167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_requirements_txt(llm_response):\n",
    "    \"\"\"\n",
    "    Extracts the main contents of the `requirements.txt` file from an LLM response.\n",
    "\n",
    "    Args:\n",
    "        llm_response (str): The response text containing the `requirements.txt` section.\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted `requirements.txt` content as a string.\n",
    "    \"\"\"\n",
    "    match = re.search(r\"```(?:\\w+\\n)?(.*?)\\n```\", llm_response, re.DOTALL)\n",
    "    return match.group(1).strip() if match else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "696042b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "requirements_txt_path = os.path.join(inference_script_dir, \"requirements.txt\")\n",
    "requirements_txt = extract_requirements_txt(response)\n",
    "write_to_file(requirements_txt_path, requirements_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20bd14b",
   "metadata": {},
   "source": [
    "## Dockerization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "df10d02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "docker_generator_llm = MyOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "11e29e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {'python_version': python_version}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e2322491",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dockerfile(options, feedback=None):\n",
    "    docker_template_file = \"/Users/neel/Developer/deploy_wizard/templates/aws_sagemaker_endpoint/Dockerfile\"\n",
    "    docker_template = file_reader(docker_template_file)\n",
    "    if feedback is None:\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a Docker expert.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f'''\n",
    "        You are an expert Dockerfile Generation Agent. Your task is to generate a **Dockerfile** for a containerized an inference script and its artifacts for AWS SageMaker Endpoint.\n",
    "\n",
    "        \n",
    "        ### Context:\n",
    "        - **Working Directory**: The Dockerfile is located inside the project directory where the model inference script, requirements file, and a serve script are located.\n",
    "        - **Python Version**: {options['python_version']}\n",
    "        - I am providing you a template for the Dockerfile, you can use it as a reference.\n",
    "        - {docker_template}\n",
    "\n",
    "\n",
    "        ### Additional Constraints:\n",
    "        - **Do not use absolute paths.**  \n",
    "        - **Assume all files are within the same directory as the Dockerfile when running `docker build .`.**\n",
    "        - **Ensure the COPY commands properly reflect this.**\n",
    "        \n",
    "        ### Expected Output:\n",
    "        Provide a complete **Dockerfile** as a code block with Dockerfile syntax highlighting.\n",
    "        No need to include entrypoint or command for copying the model file\n",
    "        '''\n",
    "            }\n",
    "        ]\n",
    "    else:\n",
    "        messages = feedback\n",
    "    response = docker_generator_llm.invoke(messages)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e0a60a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dockerfile = generate_dockerfile(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3637b05a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```Dockerfile\n",
      "# Use an appropriate base image\n",
      "FROM python:3.10.12-slim\n",
      "\n",
      "# Set environment variables\n",
      "ENV PYTHONUNBUFFERED=TRUE\n",
      "ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
      "ENV PATH=\"/opt/program:${PATH}\"\n",
      "\n",
      "# Copy the current directory contents into the container at /opt/program\n",
      "COPY . /opt/program\n",
      "\n",
      "# Set the working directory\n",
      "WORKDIR /opt/program\n",
      "\n",
      "# Install Python dependencies\n",
      "RUN pip install --no-cache-dir -r requirements.txt\n",
      "\n",
      "# Make the serve script executable\n",
      "RUN chmod +x serve\n",
      "\n",
      "# Expose the port for the FastAPI application\n",
      "EXPOSE 8080\n",
      "\n",
      "# Note: The command to run the inference script is not included as per instructions\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(raw_dockerfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2b7fdabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dockerfile(llm_response):\n",
    "    \"\"\"\n",
    "    Extracts the main Python script from an LLM response.\n",
    "\n",
    "    Args:\n",
    "        llm_response (str): The response text containing a Python code block.\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted Dockerfile, or an empty string if no script is found.\n",
    "    \"\"\"\n",
    "    match = re.search(r\"```Dockerfile\\n(.*?)\\n```\", llm_response, re.DOTALL)\n",
    "    return match.group(1) if match else \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a0a2fb50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dockerfile saved to /Users/neel/Developer/sagemaker_test/inference/Dockerfile\n",
      "# Use an appropriate base image\n",
      "FROM python:3.10.12-slim\n",
      "\n",
      "# Set environment variables\n",
      "ENV PYTHONUNBUFFERED=TRUE\n",
      "ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
      "ENV PATH=\"/opt/program:${PATH}\"\n",
      "\n",
      "# Copy the current directory contents into the container at /opt/program\n",
      "COPY . /opt/program\n",
      "\n",
      "# Set the working directory\n",
      "WORKDIR /opt/program\n",
      "\n",
      "# Install Python dependencies\n",
      "RUN pip install --no-cache-dir -r requirements.txt\n",
      "\n",
      "# Make the serve script executable\n",
      "RUN chmod +x serve\n",
      "\n",
      "# Expose the port for the FastAPI application\n",
      "EXPOSE 8080\n",
      "\n",
      "# Note: The command to run the inference script is not included as per instructions\n"
     ]
    }
   ],
   "source": [
    "docker_file_path = os.path.join(inference_script_dir, \"Dockerfile\")\n",
    "dockerfile = extract_dockerfile(raw_dockerfile)\n",
    "write_to_file(docker_file_path, dockerfile)\n",
    "print(f\"Dockerfile saved to {docker_file_path}\")    \n",
    "print(dockerfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3be877ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "docker_options = {\n",
    "    'model_dir': '/Users/neel/Developer/models',\n",
    "    'image_name': 'iris_model',\n",
    "}\n",
    "def docker_validation(options):\n",
    "    print(\"Dockerfile validation\")\n",
    "    msg = f\"\"\"Now I want to validate the dockerfile you generated. \n",
    "    I need you help to provide me instrustion on \n",
    "    1. How to build the docker image, with the name {options['image_name']}, \n",
    "    2. How to run the docker image with \n",
    "        - by mountinng my model file present inside {options['model_dir']} to /opt/ml/model\n",
    "        - providing entrypoint as the serve script\n",
    "    3. Once that is done, I want to test the docker image by running a curl command to the /ping endpoint\n",
    "    4. I also want to test the /invocations endpoint by providing a sample input\n",
    "    5. If I face any issues, I will ask you for help\n",
    "    Please provide me the instructions step by step.\"\"\"\n",
    "    while True:\n",
    "        response = docker_generator_llm.invoke(msg)\n",
    "        print(\"AI\", response)\n",
    "        input_text = input(\"Human (gg to exit): \")\n",
    "        print(\"Human: \", input_text)\n",
    "        if input_text.lower() == \"gg\":\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c1937c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dockerfile validation\n",
      "AI Certainly! Here are the detailed instructions to build, run, and test your Docker image:\n",
      "\n",
      "### Step 1: Build the Docker Image\n",
      "\n",
      "1. **Open a terminal** and navigate to the directory containing your Dockerfile and other project files.\n",
      "\n",
      "2. **Build the Docker image** with the name `iris_model` using the following command:\n",
      "\n",
      "   ```bash\n",
      "   docker build -t iris_model .\n",
      "   ```\n",
      "\n",
      "### Step 2: Run the Docker Image\n",
      "\n",
      "1. **Run the Docker container** with the model file mounted and the `serve` script as the entry point. Use the following command, replacing `/Users/neel/Developer/models` with the path to your model file on your host machine:\n",
      "\n",
      "   ```bash\n",
      "   docker run -d -p 8080:8080 --name iris_container \\\n",
      "   -v /Users/neel/Developer/models:/opt/ml/model \\\n",
      "   --entrypoint /opt/program/serve iris_model\n",
      "   ```\n",
      "\n",
      "   - `-d` runs the container in detached mode.\n",
      "   - `-p 8080:8080` maps port 8080 of the container to port 8080 on your host machine.\n",
      "   - `-v /Users/neel/Developer/models:/opt/ml/model` mounts your model file to `/opt/ml/model` inside the container.\n",
      "   - `--entrypoint /opt/program/serve` sets the `serve` script as the entry point.\n",
      "\n",
      "### Step 3: Test the Docker Image with `curl`\n",
      "\n",
      "1. **Test the `/ping` endpoint** to check if the service is running:\n",
      "\n",
      "   ```bash\n",
      "   curl http://localhost:8080/ping\n",
      "   ```\n",
      "\n",
      "   - This should return a response indicating that the service is alive (e.g., `200 OK` or a simple \"pong\" message).\n",
      "\n",
      "2. **Test the `/invocations` endpoint** by providing a sample input. Replace `sample-input.json` with the path to your JSON file containing the sample input data:\n",
      "\n",
      "   ```bash\n",
      "   curl -X POST http://localhost:8080/invocations \\\n",
      "   -H \"Content-Type: application/json\" \\\n",
      "   -d @sample-input.json\n",
      "   ```\n",
      "\n",
      "   - This should return the inference result based on the input data you provided.\n",
      "\n",
      "### Step 4: Troubleshooting\n",
      "\n",
      "- **Check Container Logs**: If you encounter any issues, you can check the logs of your running container to diagnose the problem:\n",
      "\n",
      "  ```bash\n",
      "  docker logs iris_container\n",
      "  ```\n",
      "\n",
      "- **Ensure Correct Paths**: Double-check that the paths you provide for mounting and input data are correct.\n",
      "\n",
      "- **Verify Network Settings**: Ensure that port 8080 is not blocked by any firewall or other network settings on your host machine.\n",
      "\n",
      "Feel free to ask for further assistance if you encounter any issues during these steps!\n",
      "Human:  ff\n",
      "AI Certainly! Here are the step-by-step instructions to build, run, and test your Docker image:\n",
      "\n",
      "### Step 1: Build the Docker Image\n",
      "\n",
      "1. **Open a terminal** and navigate to the directory containing your Dockerfile and other project files.\n",
      "\n",
      "2. **Build the Docker image** with the name `iris_model` using the following command:\n",
      "\n",
      "   ```bash\n",
      "   docker build -t iris_model .\n",
      "   ```\n",
      "\n",
      "### Step 2: Run the Docker Image\n",
      "\n",
      "1. **Run the Docker container** with the model file mounted and the `serve` script as the entry point. Use the following command, replacing `/Users/neel/Developer/models` with the path to your model file on your host machine:\n",
      "\n",
      "   ```bash\n",
      "   docker run -d -p 8080:8080 --name iris_container \\\n",
      "   -v /Users/neel/Developer/models:/opt/ml/model \\\n",
      "   --entrypoint /opt/program/serve iris_model\n",
      "   ```\n",
      "\n",
      "   - `-d` runs the container in detached mode.\n",
      "   - `-p 8080:8080` maps port 8080 of the container to port 8080 on your host machine.\n",
      "   - `-v /Users/neel/Developer/models:/opt/ml/model` mounts your model file to `/opt/ml/model` inside the container.\n",
      "   - `--entrypoint /opt/program/serve` sets the `serve` script as the entry point.\n",
      "\n",
      "### Step 3: Test the Docker Image with `curl`\n",
      "\n",
      "1. **Test the `/ping` endpoint** to check if the service is running:\n",
      "\n",
      "   ```bash\n",
      "   curl http://localhost:8080/ping\n",
      "   ```\n",
      "\n",
      "   - This should return a response indicating that the service is alive (e.g., `200 OK` or a simple \"pong\" message).\n",
      "\n",
      "2. **Test the `/invocations` endpoint** by providing a sample input. Create a JSON file named `sample-input.json` with your test data, and then run:\n",
      "\n",
      "   ```bash\n",
      "   curl -X POST http://localhost:8080/invocations \\\n",
      "   -H \"Content-Type: application/json\" \\\n",
      "   -d @sample-input.json\n",
      "   ```\n",
      "\n",
      "   - This should return the inference result based on the input data you provided.\n",
      "\n",
      "### Step 4: Troubleshooting\n",
      "\n",
      "- **Check Container Logs**: If you encounter any issues, you can check the logs of your running container to diagnose the problem:\n",
      "\n",
      "  ```bash\n",
      "  docker logs iris_container\n",
      "  ```\n",
      "\n",
      "- **Ensure Correct Paths**: Double-check that the paths you provide for mounting and input data are correct.\n",
      "\n",
      "- **Verify Network Settings**: Ensure that port 8080 is not blocked by any firewall or other network settings on your host machine.\n",
      "\n",
      "Feel free to ask for further assistance if you encounter any issues during these steps!\n",
      "Human:  gg\n"
     ]
    }
   ],
   "source": [
    "docker_validation(docker_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b31792",
   "metadata": {},
   "source": [
    "## Pushing to AWS ECR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0a064d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make sure you have the AWS CLI installed and configured with the necessary permissions.\n",
      "Make sure ou have an ECR repository created. In the same region as your Sagemaker\n",
      "Make sure you have Docker installed and running.\n",
      "Login to ECR\n",
      "aws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin 407683973276.dkr.ecr.us-west-2.amazonaws.com\n",
      "Tagging the locally build image for ECR\n",
      "docker tag sagemaker-test:latest 407683973276.dkr.ecr.us-west-2.amazonaws.com/sagemaker_models/iris:latest\n",
      "Pushing the image to ECR\n",
      "docker push 407683973276.dkr.ecr.us-west-2.amazonaws.com/sagemaker_models/iris:latest\n"
     ]
    }
   ],
   "source": [
    "ecr_options = {\n",
    "    'region': 'us-west-2',\n",
    "    'account_id': '407683973276',\n",
    "    'ecr_repository_name': 'sagemaker_models/iris',\n",
    "    'local_image_name': 'sagemaker-test',\n",
    "    'image_tag': 'latest'\n",
    "}\n",
    "def provide_commmands_to_push_ecr(options):\n",
    "    print(\"Make sure you have the AWS CLI installed and configured with the necessary permissions.\")\n",
    "    print(\"Make sure ou have an ECR repository created. In the same region as your Sagemaker\")\n",
    "    print(\"Make sure you have Docker installed and running.\")\n",
    "    print(\"Login to ECR\")\n",
    "    print(f\"aws ecr get-login-password --region {options['region']} | docker login --username AWS --password-stdin {options['account_id']}.dkr.ecr.{options['region']}.amazonaws.com\")\n",
    "    print(\"Tagging the locally build image for ECR\")\n",
    "    print(f\"docker tag {options['local_image_name']}:latest {options['account_id']}.dkr.ecr.{options['region']}.amazonaws.com/{options['ecr_repository_name']}:latest\")\n",
    "    print(\"Pushing the image to ECR\")\n",
    "    print(f\"docker push {options['account_id']}.dkr.ecr.{options['region']}.amazonaws.com/{options['ecr_repository_name']}:latest\")\n",
    "\n",
    "provide_commmands_to_push_ecr(ecr_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a590171f",
   "metadata": {},
   "source": [
    "## Uploading compressing and uploading the model to s3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "24d144ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "def compress_and_upload_to_s3(model_path, bucket_name, s3_key):\n",
    "    \"\"\"\n",
    "    Compress a model file to tar.gz and upload it to an S3 bucket.\n",
    "\n",
    "    Args:\n",
    "        model_path (str): Path to the model file or directory.\n",
    "        bucket_name (str): Name of the S3 bucket.\n",
    "        s3_key (str): Key (path) in the S3 bucket where the file will be uploaded.\n",
    "\n",
    "    Returns:\n",
    "        str: The S3 URI of the uploaded file.\n",
    "    \"\"\"\n",
    "    print(\"Uploading model to S3...\")\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"The specified model path does not exist: {model_path}\")\n",
    "    \n",
    "    # Compress the model to a tar.gz file\n",
    "    compressed_file = f\"{os.path.splitext(model_path)[0]}.tar.gz\"\n",
    "    try:\n",
    "        subprocess.run(['tar', '-czf', compressed_file, '-C', os.path.dirname(model_path), os.path.basename(model_path)],\n",
    "                       check=True)\n",
    "        print(f\"Model compressed to: {compressed_file}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        raise RuntimeError(f\"Error compressing the model: {e}\")\n",
    "    \n",
    "    # Upload the compressed file to S3\n",
    "    try:\n",
    "        s3_uri = f\"s3://{bucket_name}/{s3_key}\"\n",
    "        subprocess.run(['aws', 's3', 'cp', compressed_file, s3_uri], check=True)\n",
    "        print(f\"File uploaded to S3: {s3_uri}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        raise RuntimeError(f\"Error uploading the file to S3: {e}\")\n",
    "    finally:\n",
    "        # Clean up the compressed file\n",
    "        if os.path.exists(compressed_file):\n",
    "            os.remove(compressed_file)\n",
    "            print(f\"Temporary file removed: {compressed_file}\")\n",
    "    \n",
    "    return s3_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e562d56a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading model to S3...\n",
      "Model compressed to: /Users/neel/Developer/models/iris_model.tar.gz\n",
      "upload: ../models/iris_model.tar.gz to s3://neel-sagemaker-models/iris_model/model2.tar.gz\n",
      "File uploaded to S3: s3://neel-sagemaker-models/iris_model/model2.tar.gz\n",
      "Temporary file removed: /Users/neel/Developer/models/iris_model.tar.gz\n",
      "Model uploaded to S3 at: s3://neel-sagemaker-models/iris_model/model2.tar.gz\n"
     ]
    }
   ],
   "source": [
    "s3_bucket_name = \"neel-sagemaker-models\"\n",
    "s3_prefix = \"iris_model\"\n",
    "s3_key = f\"{s3_prefix}/model2.tar.gz\"\n",
    "model_path = \"/Users/neel/Developer/models/iris_model.pkl\"\n",
    "s3_uri = compress_and_upload_to_s3(model_path, s3_bucket_name, s3_key)\n",
    "print(f\"Model uploaded to S3 at: {s3_uri}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a142b3a",
   "metadata": {},
   "source": [
    "## Terraform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "96f7a8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "terrraform_generator_llm = MyOpenAI(model=\"gpt-4o\")\n",
    "terraform_dir = \"/Users/neel/Developer/sagemaker_test/terraform\"\n",
    "terraform_critical_llm = MyOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "bbf6a282",
   "metadata": {},
   "outputs": [],
   "source": [
    "saggemaker_terraform_options = {\n",
    "    'region': 'us-west-2',\n",
    "    's3_uri': s3_uri,\n",
    "    'instance_type': 'ml.t2.mdeium',\n",
    "    'model_name': 'iris-model',\n",
    "    'ecr_image': '407683973276.dkr.ecr.us-west-2.amazonaws.com/sagemaker_models/iris:latest',\n",
    "    \"model_in_s3\": s3_uri,\n",
    "}\n",
    "\n",
    "\n",
    "def terraform_generator_actor(options, feedback=None):\n",
    "    terraform_template_file = \"/Users/neel/Developer/deploy_wizard/templates/aws_sagemaker_endpoint/main.tf\"\n",
    "    terraform_template = file_reader(terraform_template_file)\n",
    "    if feedback is None:\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a Terraform expert.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f'''\n",
    "        You are an expert Terraform Generation Agent. Your task is to generate a **Terraform** script for deploying a model on AWS SageMaker Endpoint.\n",
    "        ### Context:\n",
    "        I need a terraform file to deploy a model on AWS SageMaker Endpoint. The terraform code should be able to deploy the following:\n",
    "        1. An IAM role with the necessary permissions for SageMaker and S3, this role will be be used by the model.\n",
    "        2. A SageMaker model with name {options['model_name']} that uses the ECR image {options['ecr_image']} and S3 URI {options['model_in_s3']}.\n",
    "        3. An endpoint configuration for the SageMaker model.\n",
    "        4. An endpoint for the SageMaker model.\n",
    "        5. For IAM role and its polcies, use what is there in the template. You have to give it a name that is similar to the model name.\n",
    "        6. Use the instance type {options['instance_type']} for the endpoint.\n",
    "        7. Use the region {options['region']} for the resources.\n",
    "        8. Overwrite the values in the template with the values provided in the options.\n",
    "\n",
    "        Here is a template for the terraform file, you can use it as a reference.\n",
    "        {terraform_template}'''}\n",
    "        ]\n",
    "    else:\n",
    "        messages = feedback\n",
    "    response = terrraform_generator_llm.invoke(messages)\n",
    "    return response\n",
    "\n",
    "def terraform_critic_evaluator(options):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a Terraform expert.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"\"\"\n",
    "        You are reviewing a terraform script generated for deploying a model on AWS SageMaker Endpoint.\n",
    "\n",
    "        ### Task:\n",
    "        - Analyze the script for correctness, best practices, and completeness.\n",
    "        - Identify any missing components or improvements.\n",
    "        - If the script is perfect, respond with 'No changes needed.'\n",
    "        - If changes are required, specify what needs to be improved.\n",
    "        - Dont menion the IAM role, it is already created and will be used by the model.\n",
    "        \n",
    "\n",
    "        ### Script to review:\n",
    "        ```\n",
    "        {response}\n",
    "        ```\n",
    "\n",
    "        ### Expected Output:\n",
    "        - Either 'No changes needed.' OR a detailed improvement plan.\n",
    "        \"\"\"}\n",
    "    ]\n",
    "    \n",
    "    return terraform_critical_llm.invoke(messages)\n",
    "\n",
    "def actor_critic_terraform_script(options):\n",
    "        \"\"\"\n",
    "        Runs the actor-critic loop:\n",
    "        - Actor generates a terraform script.\n",
    "        - Critic evaluates the script.\n",
    "        - If the script is sufficient, exit early.\n",
    "        - Otherwise, Actor refines the script based on feedback.\n",
    "        \"\"\"\n",
    "        feedback = None  # No feedback initially\n",
    "\n",
    "        for _ in range(1):  # Max iterations: 1\n",
    "            # Step 1: Actor generates inference script\n",
    "            terraform_script = terraform_generator_actor(options, feedback)\n",
    "\n",
    "            # Step 2: Critic evaluates the script\n",
    "            # critic_feedback = terraform_critic_evaluator(terraform_script)\n",
    "\n",
    "            # if \"no changes needed\" in critic_feedback.lower():\n",
    "            #     print(\"âœ… Terraform script is satisfactory. Exiting early.\")\n",
    "            #     return terraform_script  # Early exit if script is sufficient\n",
    "\n",
    "            # # Step 3: Actor refines script using critic's feedback\n",
    "            # feedback = [\n",
    "            #     {\"role\": \"assistant\", \"content\": terraform_script},\n",
    "            #     {\"role\": \"user\", \"content\": f\"Revise based on this feedback:\\n{critic_feedback}\"}\n",
    "            # ]\n",
    "\n",
    "        print(\"ðŸ”„ Max iterations reached. Returning final script.\")\n",
    "        return terraform_script\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "9785ca07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Max iterations reached. Returning final script.\n"
     ]
    }
   ],
   "source": [
    "response = actor_critic_terraform_script(saggemaker_terraform_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "6ed07fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is the Terraform script tailored to your requirements for deploying a model on AWS SageMaker Endpoint. The script includes the creation of an IAM role with necessary permissions, a SageMaker model, an endpoint configuration, and an endpoint. The values have been customized based on your specifications.\n",
      "\n",
      "```hcl\n",
      "terraform {\n",
      "  required_providers {\n",
      "    aws = {\n",
      "      source  = \"hashicorp/aws\"\n",
      "      version = \">= 5.45.0\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "###############################################################################\n",
      "# VARIABLES\n",
      "###############################################################################\n",
      "\n",
      "variable \"aws_region\" {\n",
      "  description = \"AWS region to deploy resources in\"\n",
      "  type        = string\n",
      "  default     = \"us-west-2\"\n",
      "}\n",
      "\n",
      "variable \"execution_role_name\" {\n",
      "  description = \"IAM role name that SageMaker will assume\"\n",
      "  type        = string\n",
      "  default     = \"iris-model-execution-role\"\n",
      "}\n",
      "\n",
      "variable \"model_name\" {\n",
      "  description = \"Name of the SageMaker Model resource\"\n",
      "  type        = string\n",
      "  default     = \"iris-model\"\n",
      "}\n",
      "\n",
      "variable \"ecr_image_uri\" {\n",
      "  description = \"ECR image URI for the inference container\"\n",
      "  type        = string\n",
      "  default     = \"407683973276.dkr.ecr.us-west-2.amazonaws.com/sagemaker_models/iris:latest\"\n",
      "}\n",
      "\n",
      "variable \"model_data_s3_uri\" {\n",
      "  description = \"S3 URI of the model artifact tar.gz\"\n",
      "  type        = string\n",
      "  default     = \"s3://neel-sagemaker-models/iris_model/model2.tar.gz\"\n",
      "}\n",
      "\n",
      "variable \"container_hostname\" {\n",
      "  description = \"Hostname inside the container\"\n",
      "  type        = string\n",
      "  default     = \"model-server\"\n",
      "}\n",
      "\n",
      "variable \"endpoint_config_name\" {\n",
      "  description = \"Name of the SageMaker endpoint configuration\"\n",
      "  type        = string\n",
      "  default     = \"iris-endpoint-config\"\n",
      "}\n",
      "\n",
      "variable \"production_variant_name\" {\n",
      "  description = \"Variant name in the endpoint configuration\"\n",
      "  type        = string\n",
      "  default     = \"AllTraffic\"\n",
      "}\n",
      "\n",
      "variable \"initial_instance_count\" {\n",
      "  description = \"Number of instances for the endpoint\"\n",
      "  type        = number\n",
      "  default     = 1\n",
      "}\n",
      "\n",
      "variable \"instance_type\" {\n",
      "  description = \"EC2 instance type for the endpoint\"\n",
      "  type        = string\n",
      "  default     = \"ml.t2.medium\"\n",
      "}\n",
      "\n",
      "variable \"endpoint_name\" {\n",
      "  description = \"Name of the SageMaker endpoint\"\n",
      "  type        = string\n",
      "  default     = \"iris-endpoint\"\n",
      "}\n",
      "\n",
      "###############################################################################\n",
      "# PROVIDER\n",
      "###############################################################################\n",
      "\n",
      "provider \"aws\" {\n",
      "  region = var.aws_region\n",
      "}\n",
      "\n",
      "###############################################################################\n",
      "# IAM ROLE & STATIC POLICY ATTACHMENTS\n",
      "###############################################################################\n",
      "\n",
      "resource \"aws_iam_role\" \"sagemaker_execution_role\" {\n",
      "  name = var.execution_role_name\n",
      "\n",
      "  assume_role_policy = jsonencode({\n",
      "    Version = \"2012-10-17\"\n",
      "    Statement = [{\n",
      "      Effect    = \"Allow\"\n",
      "      Principal = { Service = \"sagemaker.amazonaws.com\" }\n",
      "      Action    = \"sts:AssumeRole\"\n",
      "    }]\n",
      "  })\n",
      "}\n",
      "\n",
      "resource \"aws_iam_role_policy_attachment\" \"attach_s3_full_access\" {\n",
      "  role       = aws_iam_role.sagemaker_execution_role.name\n",
      "  policy_arn = \"arn:aws:iam::aws:policy/AmazonS3FullAccess\"\n",
      "}\n",
      "\n",
      "resource \"aws_iam_role_policy_attachment\" \"attach_sagemaker_full_access\" {\n",
      "  role       = aws_iam_role.sagemaker_execution_role.name\n",
      "  policy_arn = \"arn:aws:iam::aws:policy/AmazonSageMakerFullAccess\"\n",
      "}\n",
      "\n",
      "###############################################################################\n",
      "# SAGEMAKER MODEL\n",
      "###############################################################################\n",
      "\n",
      "resource \"aws_sagemaker_model\" \"model\" {\n",
      "  name               = var.model_name\n",
      "  execution_role_arn = aws_iam_role.sagemaker_execution_role.arn\n",
      "\n",
      "  primary_container {\n",
      "    image              = var.ecr_image_uri\n",
      "    model_data_url     = var.model_data_s3_uri\n",
      "    container_hostname = var.container_hostname\n",
      "  }\n",
      "}\n",
      "\n",
      "###############################################################################\n",
      "# ENDPOINT CONFIGURATION\n",
      "###############################################################################\n",
      "\n",
      "resource \"aws_sagemaker_endpoint_configuration\" \"config\" {\n",
      "  name = var.endpoint_config_name\n",
      "\n",
      "  production_variants {\n",
      "    variant_name           = var.production_variant_name\n",
      "    model_name             = aws_sagemaker_model.model.name\n",
      "    initial_instance_count = var.initial_instance_count\n",
      "    instance_type          = var.instance_type\n",
      "  }\n",
      "}\n",
      "\n",
      "###############################################################################\n",
      "# SAGEMAKER ENDPOINT\n",
      "###############################################################################\n",
      "\n",
      "resource \"aws_sagemaker_endpoint\" \"endpoint\" {\n",
      "  name                 = var.endpoint_name\n",
      "  endpoint_config_name = aws_sagemaker_endpoint_configuration.config.name\n",
      "}\n",
      "```\n",
      "\n",
      "This script sets up the necessary AWS resources in the `us-west-2` region, using the specified ECR image and S3 model data URI. The IAM role is configured with the required permissions for SageMaker and S3, and the endpoint is set to use the `ml.t2.medium` instance type.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb1bf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool()\n",
    "def save_file_tool(file_path: str, content: str) -> str:\n",
    "    \"\"\"Save content to a file at the given path.\"\"\"\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(content)\n",
    "    return f\"Saved to {file_path}\"\n",
    "\n",
    "def extract_and_save_inference_files(response_str: str, save_dir: str = ''):\n",
    "        llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "        tools = [save_file_tool]\n",
    "\n",
    "        agent = initialize_agent(\n",
    "            tools=tools,\n",
    "            llm=llm,\n",
    "            agent=AgentType.OPENAI_FUNCTIONS,\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "        response = agent.invoke(\n",
    "            f\"\"\"\n",
    "            1. Extract all terraform code from the response string.\n",
    "            2. For each extracted script:\n",
    "                 - save it with appropriate name and extension like main.tf, etc. at {terraform_dir}\n",
    "            3\n",
    "            4. Use the `save_file_tool` tool to save the content.\n",
    "            5. Do not output anything else except confirming tool usage.\n",
    "\n",
    "            ### Text:\n",
    "            {response_str}\n",
    "            \"\"\"\n",
    "        )\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e1ebfbfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `save_file_tool` with `{'file_path': '/Users/neel/Developer/sagemaker_test/terraform/main.tf', 'content': '###############################################################################\\n# Terraform script for deploying a model on AWS SageMaker Endpoint\\n###############################################################################\\n\\ntf {\\n  required_providers {\\n    aws = {\\n      source  = \"hashicorp/aws\"\\n      version = \">= 5.45.0\"\\n    }\\n  }\\n}\\n\\n###############################################################################\\n# VARIABLES\\n###############################################################################\\n\\nvariable \"aws_region\" {\\n  description = \"AWS region to deploy resources in\"\\n  type        = string\\n  default     = \"us-west-2\"\\n}\\n\\nvariable \"execution_role_name\" {\\n  description = \"IAM role name that SageMaker will assume\"\\n  type        = string\\n  default     = \"iris-model-execution-role\"\\n}\\n\\nvariable \"model_name\" {\\n  description = \"Name of the SageMaker Model resource\"\\n  type        = string\\n  default     = \"iris-model\"\\n}\\n\\nvariable \"ecr_image_uri\" {\\n  description = \"ECR image URI for the inference container\"\\n  type        = string\\n  default     = \"407683973276.dkr.ecr.us-west-2.amazonaws.com/sagemaker_models/iris:latest\"\\n}\\n\\nvariable \"model_data_s3_uri\" {\\n  description = \"S3 URI of the model artifact tar.gz\"\\n  type        = string\\n  default     = \"s3://neel-sagemaker-models/iris_model/model2.tar.gz\"\\n}\\n\\nvariable \"container_hostname\" {\\n  description = \"Hostname inside the container\"\\n  type        = string\\n  default     = \"model-server\"\\n}\\n\\nvariable \"endpoint_config_name\" {\\n  description = \"Name of the SageMaker endpoint configuration\"\\n  type        = string\\n  default     = \"iris-endpoint-config\"\\n}\\n\\nvariable \"production_variant_name\" {\\n  description = \"Variant name in the endpoint configuration\"\\n  type        = string\\n  default     = \"AllTraffic\"\\n}\\n\\nvariable \"initial_instance_count\" {\\n  description = \"Number of instances for the endpoint\"\\n  type        = number\\n  default     = 1\\n}\\n\\nvariable \"instance_type\" {\\n  description = \"EC2 instance type for the endpoint\"\\n  type        = string\\n  default     = \"ml.t3.medium\"\\n}\\n\\nvariable \"endpoint_name\" {\\n  description = \"Name of the SageMaker endpoint\"\\n  type        = string\\n  default     = \"iris-endpoint\"\\n}\\n\\n###############################################################################\\n# PROVIDER\\n###############################################################################\\n\\nprovider \"aws\" {\\n  region = var.aws_region\\n}\\n\\n###############################################################################\\n# IAM ROLE & STATIC POLICY ATTACHMENTS\\n###############################################################################\\n\\nresource \"aws_iam_role\" \"sagemaker_execution_role\" {\\n  name = var.execution_role_name\\n\\n  assume_role_policy = jsonencode({\\n    Version = \"2012-10-17\"\\n    Statement = [{\\n      Effect    = \"Allow\"\\n      Principal = { Service = \"sagemaker.amazonaws.com\" }\\n      Action    = \"sts:AssumeRole\"\\n    }]\\n  })\\n}\\n\\nresource \"aws_iam_role_policy_attachment\" \"attach_s3_full_access\" {\\n  role       = aws_iam_role.sagemaker_execution_role.name\\n  policy_arn = \"arn:aws:iam::aws:policy/AmazonS3FullAccess\"\\n}\\n\\nresource \"aws_iam_role_policy_attachment\" \"attach_sagemaker_full_access\" {\\n  role       = aws_iam_role.sagemaker_execution_role.name\\n  policy_arn = \"arn:aws:iam::aws:policy/AmazonSageMakerFullAccess\"\\n}\\n\\n###############################################################################\\n# SAGEMAKER MODEL\\n###############################################################################\\n\\nresource \"aws_sagemaker_model\" \"model\" {\\n  name               = var.model_name\\n  execution_role_arn = aws_iam_role.sagemaker_execution_role.arn\\n\\n  primary_container {\\n    image              = var.ecr_image_uri\\n    model_data_url     = var.model_data_s3_uri\\n    container_hostname = var.container_hostname\\n  }\\n}\\n\\n###############################################################################\\n# ENDPOINT CONFIGURATION\\n###############################################################################\\n\\nresource \"aws_sagemaker_endpoint_configuration\" \"config\" {\\n  name = var.endpoint_config_name\\n\\n  production_variants {\\n    variant_name           = var.production_variant_name\\n    model_name             = aws_sagemaker_model.model.name\\n    initial_instance_count = var.initial_instance_count\\n    instance_type          = var.instance_type\\n  }\\n}\\n\\n###############################################################################\\n# SAGEMAKER ENDPOINT\\n###############################################################################\\n\\nresource \"aws_sagemaker_endpoint\" \"endpoint\" {\\n  name                 = var.endpoint_name\\n  endpoint_config_name = aws_sagemaker_endpoint_configuration.config.name\\n}'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mSaved to /Users/neel/Developer/sagemaker_test/terraform/main.tf\u001b[0m\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': '\\n            1. Extract all terraform code from the response string.\\n            2. For each extracted script:\\n                 - save it with appropriate name and extension like main.tf, etc. at /Users/neel/Developer/sagemaker_test/terraform\\n            3. Use the `save_file_tool` tool to save the content.\\n            4. Do not output anything else except confirming tool usage.\\n\\n            ### Text:\\n            Below is the Terraform script tailored to your requirements for deploying a model on AWS SageMaker Endpoint:\\n\\n```hcl\\n###############################################################################\\n# Terraform script for deploying a model on AWS SageMaker Endpoint\\n###############################################################################\\n\\nterraform {\\n  required_providers {\\n    aws = {\\n      source  = \"hashicorp/aws\"\\n      version = \">= 5.45.0\"\\n    }\\n  }\\n}\\n\\n###############################################################################\\n# VARIABLES\\n###############################################################################\\n\\nvariable \"aws_region\" {\\n  description = \"AWS region to deploy resources in\"\\n  type        = string\\n  default     = \"us-west-2\"\\n}\\n\\nvariable \"execution_role_name\" {\\n  description = \"IAM role name that SageMaker will assume\"\\n  type        = string\\n  default     = \"iris-model-execution-role\"\\n}\\n\\nvariable \"model_name\" {\\n  description = \"Name of the SageMaker Model resource\"\\n  type        = string\\n  default     = \"iris-model\"\\n}\\n\\nvariable \"ecr_image_uri\" {\\n  description = \"ECR image URI for the inference container\"\\n  type        = string\\n  default     = \"407683973276.dkr.ecr.us-west-2.amazonaws.com/sagemaker_models/iris:latest\"\\n}\\n\\nvariable \"model_data_s3_uri\" {\\n  description = \"S3 URI of the model artifact tar.gz\"\\n  type        = string\\n  default     = \"s3://neel-sagemaker-models/iris_model/model2.tar.gz\"\\n}\\n\\nvariable \"container_hostname\" {\\n  description = \"Hostname inside the container\"\\n  type        = string\\n  default     = \"model-server\"\\n}\\n\\nvariable \"endpoint_config_name\" {\\n  description = \"Name of the SageMaker endpoint configuration\"\\n  type        = string\\n  default     = \"iris-endpoint-config\"\\n}\\n\\nvariable \"production_variant_name\" {\\n  description = \"Variant name in the endpoint configuration\"\\n  type        = string\\n  default     = \"AllTraffic\"\\n}\\n\\nvariable \"initial_instance_count\" {\\n  description = \"Number of instances for the endpoint\"\\n  type        = number\\n  default     = 1\\n}\\n\\nvariable \"instance_type\" {\\n  description = \"EC2 instance type for the endpoint\"\\n  type        = string\\n  default     = \"ml.t3.medium\"\\n}\\n\\nvariable \"endpoint_name\" {\\n  description = \"Name of the SageMaker endpoint\"\\n  type        = string\\n  default     = \"iris-endpoint\"\\n}\\n\\n###############################################################################\\n# PROVIDER\\n###############################################################################\\n\\nprovider \"aws\" {\\n  region = var.aws_region\\n}\\n\\n###############################################################################\\n# IAM ROLE & STATIC POLICY ATTACHMENTS\\n###############################################################################\\n\\nresource \"aws_iam_role\" \"sagemaker_execution_role\" {\\n  name = var.execution_role_name\\n\\n  assume_role_policy = jsonencode({\\n    Version = \"2012-10-17\"\\n    Statement = [{\\n      Effect    = \"Allow\"\\n      Principal = { Service = \"sagemaker.amazonaws.com\" }\\n      Action    = \"sts:AssumeRole\"\\n    }]\\n  })\\n}\\n\\nresource \"aws_iam_role_policy_attachment\" \"attach_s3_full_access\" {\\n  role       = aws_iam_role.sagemaker_execution_role.name\\n  policy_arn = \"arn:aws:iam::aws:policy/AmazonS3FullAccess\"\\n}\\n\\nresource \"aws_iam_role_policy_attachment\" \"attach_sagemaker_full_access\" {\\n  role       = aws_iam_role.sagemaker_execution_role.name\\n  policy_arn = \"arn:aws:iam::aws:policy/AmazonSageMakerFullAccess\"\\n}\\n\\n###############################################################################\\n# SAGEMAKER MODEL\\n###############################################################################\\n\\nresource \"aws_sagemaker_model\" \"model\" {\\n  name               = var.model_name\\n  execution_role_arn = aws_iam_role.sagemaker_execution_role.arn\\n\\n  primary_container {\\n    image              = var.ecr_image_uri\\n    model_data_url     = var.model_data_s3_uri\\n    container_hostname = var.container_hostname\\n  }\\n}\\n\\n###############################################################################\\n# ENDPOINT CONFIGURATION\\n###############################################################################\\n\\nresource \"aws_sagemaker_endpoint_configuration\" \"config\" {\\n  name = var.endpoint_config_name\\n\\n  production_variants {\\n    variant_name           = var.production_variant_name\\n    model_name             = aws_sagemaker_model.model.name\\n    initial_instance_count = var.initial_instance_count\\n    instance_type          = var.instance_type\\n  }\\n}\\n\\n###############################################################################\\n# SAGEMAKER ENDPOINT\\n###############################################################################\\n\\nresource \"aws_sagemaker_endpoint\" \"endpoint\" {\\n  name                 = var.endpoint_name\\n  endpoint_config_name = aws_sagemaker_endpoint_configuration.config.name\\n}\\n```\\n\\nThis script sets up the necessary IAM role, SageMaker model, endpoint configuration, and endpoint using the specified parameters. Make sure to have the necessary permissions and AWS credentials configured to deploy these resources.\\n            ',\n",
       " 'output': ''}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_and_save_inference_files(response, terraform_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "f61dbbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def instructions_to_run_terraform():\n",
    "    message = f\"Can you provide me the instructions to run the terraform script you generated? I want to know the commands to run in order to deploy the model on AWS SageMaker Endpoint. I also want to know how to destroy the resources once I am done.\"\n",
    "    response=terrraform_generator_llm.invoke(message)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5cc6b779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Below are the step-by-step instructions to run the Terraform script and deploy your model on AWS SageMaker Endpoint, as well as how to destroy the resources when you're done.\n",
      "\n",
      "### Prerequisites\n",
      "\n",
      "1. **Install Terraform**: Ensure that Terraform is installed on your machine. You can download it from the [Terraform website](https://www.terraform.io/downloads.html).\n",
      "\n",
      "2. **AWS CLI Configuration**: Make sure you have the AWS CLI installed and configured with the necessary credentials. You can configure it using:\n",
      "   ```bash\n",
      "   aws configure\n",
      "   ```\n",
      "   This will prompt you to enter your AWS Access Key, Secret Key, region, and output format.\n",
      "\n",
      "3. **Terraform Initialization**: Ensure you have a working directory where your Terraform script (`main.tf`) is saved.\n",
      "\n",
      "### Running the Terraform Script\n",
      "\n",
      "1. **Navigate to the Directory**: Open your terminal and navigate to the directory containing your Terraform script.\n",
      "   ```bash\n",
      "   cd path/to/your/terraform/directory\n",
      "   ```\n",
      "\n",
      "2. **Initialize Terraform**: This command initializes the directory, downloads the necessary provider plugins, and sets up the backend.\n",
      "   ```bash\n",
      "   terraform init\n",
      "   ```\n",
      "\n",
      "3. **Plan the Deployment**: This command creates an execution plan, showing what actions Terraform will take to change the infrastructure.\n",
      "   ```bash\n",
      "   terraform plan\n",
      "   ```\n",
      "\n",
      "4. **Apply the Deployment**: This command applies the changes required to reach the desired state of the configuration.\n",
      "   ```bash\n",
      "   terraform apply\n",
      "   ```\n",
      "   - You will be prompted to confirm the action. Type `yes` to proceed.\n",
      "\n",
      "5. **Verify Deployment**: Once the apply command completes, you can verify the deployment in the AWS Management Console under SageMaker.\n",
      "\n",
      "### Destroying the Resources\n",
      "\n",
      "When you are done with the resources and want to clean up, you can destroy them using the following command:\n",
      "\n",
      "1. **Destroy the Resources**: This command will remove all the resources defined in your Terraform configuration.\n",
      "   ```bash\n",
      "   terraform destroy\n",
      "   ```\n",
      "   - You will be prompted to confirm the action. Type `yes` to proceed.\n",
      "\n",
      "### Additional Tips\n",
      "\n",
      "- **State File**: Terraform maintains a state file (`terraform.tfstate`) in your working directory. This file is crucial for tracking the resources it manages. Ensure it is not deleted or modified manually.\n",
      "\n",
      "- **Version Control**: Consider using version control (e.g., Git) to manage your Terraform scripts, especially if you plan to make changes or collaborate with others.\n",
      "\n",
      "- **Environment Variables**: If you prefer not to hardcode AWS credentials, you can set them as environment variables:\n",
      "  ```bash\n",
      "  export AWS_ACCESS_KEY_ID=\"your-access-key-id\"\n",
      "  export AWS_SECRET_ACCESS_KEY=\"your-secret-access-key\"\n",
      "  ```\n",
      "\n",
      "By following these instructions, you should be able to deploy and manage your AWS SageMaker resources using Terraform effectively.\n"
     ]
    }
   ],
   "source": [
    "instructions_to_run_terraform()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b845c881",
   "metadata": {},
   "source": [
    "## Integration testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "2b5dfe10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aws_helper.sagemaker_endpoint import DeploySagemakerEndpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "7d340598",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = DeploySagemakerEndpoint(actor_critic_iterations=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad18677",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.get_inferece_script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "13adf48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating requirements.txt...\n"
     ]
    }
   ],
   "source": [
    "a.get_requirements_txt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ec56f248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Dockerfile...\n",
      "Saving Dockerfile to: /Users/neel/Developer/sagemaker_test2/inference/Dockerfile\n",
      "Dockerfile generated successfully.\n",
      "Dockerfile validation\n",
      "AI Certainly! Here are the step-by-step instructions to build, run, and test your Docker image:\n",
      "\n",
      "### Step 1: Build the Docker Image\n",
      "\n",
      "1. Open a terminal and navigate to the directory containing your Dockerfile and other project files.\n",
      "2. Run the following command to build the Docker image with the name `iris_sagemaker`:\n",
      "\n",
      "   ```bash\n",
      "   docker build -t iris_sagemaker .\n",
      "   ```\n",
      "\n",
      "### Step 2: Run the Docker Image\n",
      "\n",
      "1. Use the following command to run the Docker image, mounting your model file directory and setting the entrypoint to the `serve` script:\n",
      "\n",
      "   ```bash\n",
      "   docker run -d --name iris_container -p 8080:8080 -v /Users/neel/Developer/models:/opt/ml/model --entrypoint /opt/program/serve iris_sagemaker\n",
      "   ```\n",
      "\n",
      "   - `-d`: Runs the container in detached mode.\n",
      "   - `--name iris_container`: Names the running container `iris_container`.\n",
      "   - `-p 8080:8080`: Maps port 8080 of the container to port 8080 on your host machine.\n",
      "   - `-v /Users/neel/Developer/models:/opt/ml/model`: Mounts the host directory `/Users/neel/Developer/models` to `/opt/ml/model` inside the container.\n",
      "   - `--entrypoint /opt/program/serve`: Sets the entrypoint to the `serve` script.\n",
      "\n",
      "### Step 3: Test the /ping Endpoint\n",
      "\n",
      "1. Use `curl` to test the `/ping` endpoint to ensure the service is running:\n",
      "\n",
      "   ```bash\n",
      "   curl -X GET http://localhost:8080/ping\n",
      "   ```\n",
      "\n",
      "   - You should receive a response indicating the service is healthy (e.g., \"pong\" or a 200 OK status).\n",
      "\n",
      "### Step 4: Test the /invocations Endpoint\n",
      "\n",
      "1. Prepare a sample input file or data to send to the `/invocations` endpoint. For example, create a JSON file named `input.json` with your test data.\n",
      "2. Use `curl` to send a POST request to the `/invocations` endpoint:\n",
      "\n",
      "   ```bash\n",
      "   curl -X POST http://localhost:8080/invocations -H \"Content-Type: application/json\" -d @input.json\n",
      "   ```\n",
      "\n",
      "   - Replace `input.json` with the path to your actual input file.\n",
      "   - You should receive a response with the inference results.\n",
      "\n",
      "### Step 5: Troubleshooting\n",
      "\n",
      "If you encounter any issues during these steps, please provide the error messages or describe the problem, and I'll be happy to help you troubleshoot further.\n",
      "Human:  gg\n",
      "To push the Docker image to AWS ECR, please provide the following information:\n",
      "Make sure you have the AWS CLI installed and configured with the necessary permissions.\n",
      "Make sure ou have an ECR repository created. In the same region as your Sagemaker\n",
      "Make sure you have Docker installed and running.\n",
      "Login to ECR\n",
      "aws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin 407683973276.dkr.ecr.us-west-2.amazonaws.com\n",
      "Tagging the locally build image for ECR\n",
      "docker tag None:latest 407683973276.dkr.ecr.us-west-2.amazonaws.com/None:latest\n",
      "Pushing the image to ECR\n",
      "docker push 407683973276.dkr.ecr.us-west-2.amazonaws.com/None:latest\n"
     ]
    }
   ],
   "source": [
    "a.dockerizing_the_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "9a5e6d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.ecr_image_full_name = \"407683973276.dkr.ecr.us-west-2.amazonaws.com/sagemaker_models/iris:latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "2097fb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.ecr_repo_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "a8241c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To upload the model to AWS S3, please provide the following information:\n",
      "Uploading model to S3...\n",
      "Model compressed to: /Users/neel/Developer/models/iris_model.tar.gz\n",
      "upload: ../models/iris_model.tar.gz to s3://neel-sagemaker-models/iris_model2/model.tar.gz\n",
      "File uploaded to S3: s3://neel-sagemaker-models/iris_model2/model.tar.gz\n",
      "Temporary file removed: /Users/neel/Developer/models/iris_model.tar.gz\n",
      "Model uploaded to S3 at: s3://neel-sagemaker-models/iris_model2/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "a.upload_model_to_s3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "ba43d8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To deploy the model on AWS SageMaker Endpoint, please provide the following information:\n",
      "ðŸ”„ Max iterations reached. Returning final script.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `save_file_tool` with `{'file_path': '/Users/neel/Developer/sagemaker_test2/terraform/main.tf', 'content': '###############################################################################\\n# VARIABLES\\n###############################################################################\\n\\nvariable \"aws_region\" {\\n  description = \"AWS region to deploy resources in\"\\n  type        = string\\n  default     = \"us-west-2\"\\n}\\n\\nvariable \"execution_role_name\" {\\n  description = \"IAM role name that SageMaker will assume\"\\n  type        = string\\n  default     = \"iris2-sagemaker-execution-role\"\\n}\\n\\nvariable \"model_name\" {\\n  description = \"Name of the SageMaker Model resource\"\\n  type        = string\\n  default     = \"iris2\"\\n}\\n\\nvariable \"ecr_image_uri\" {\\n  description = \"ECR image URI for the inference container\"\\n  type        = string\\n  default     = \"407683973276.dkr.ecr.us-west-2.amazonaws.com/sagemaker_models/iris:latest\"\\n}\\n\\nvariable \"model_data_s3_uri\" {\\n  description = \"S3 URI of the model artifact tar.gz\"\\n  type        = string\\n  default     = \"s3://neel-sagemaker-models/iris_model2/model.tar.gz\"\\n}\\n\\nvariable \"container_hostname\" {\\n  description = \"Hostname inside the container\"\\n  type        = string\\n  default     = \"model-server\"\\n}\\n\\nvariable \"endpoint_config_name\" {\\n  description = \"Name of the SageMaker endpoint configuration\"\\n  type        = string\\n  default     = \"iris2-endpoint-config\"\\n}\\n\\nvariable \"production_variant_name\" {\\n  description = \"Variant name in the endpoint configuration\"\\n  type        = string\\n  default     = \"AllTraffic\"\\n}\\n\\nvariable \"initial_instance_count\" {\\n  description = \"Number of instances for the endpoint\"\\n  type        = number\\n  default     = 1\\n}\\n\\nvariable \"instance_type\" {\\n  description = \"EC2 instance type for the endpoint\"\\n  type        = string\\n  default     = \"ml.t2.medium\"\\n}\\n\\nvariable \"endpoint_name\" {\\n  description = \"Name of the SageMaker endpoint\"\\n  type        = string\\n  default     = \"iris2-endpoint\"\\n}\\n\\n###############################################################################\\n# PROVIDER\\n###############################################################################\\n\\nprovider \"aws\" {\\n  region = var.aws_region\\n}\\n\\n###############################################################################\\n# IAM ROLE & STATIC POLICY ATTACHMENTS\\n###############################################################################\\n\\nresource \"aws_iam_role\" \"sagemaker_execution_role\" {\\n  name = var.execution_role_name\\n\\n  assume_role_policy = jsonencode({\\n    Version = \"2012-10-17\"\\n    Statement = [{\\n      Effect    = \"Allow\"\\n      Principal = { Service = \"sagemaker.amazonaws.com\" }\\n      Action    = \"sts:AssumeRole\"\\n    }]\\n  })\\n}\\n\\nresource \"aws_iam_role_policy_attachment\" \"attach_s3_full_access\" {\\n  role       = aws_iam_role.sagemaker_execution_role.name\\n  policy_arn = \"arn:aws:iam::aws:policy/AmazonS3FullAccess\"\\n}\\n\\nresource \"aws_iam_role_policy_attachment\" \"attach_sagemaker_full_access\" {\\n  role       = aws_iam_role.sagemaker_execution_role.name\\n  policy_arn = \"arn:aws:iam::aws:policy/AmazonSageMakerFullAccess\"\\n}\\n\\n###############################################################################\\n# SAGEMAKER MODEL\\n###############################################################################\\n\\nresource \"aws_sagemaker_model\" \"model\" {\\n  name               = var.model_name\\n  execution_role_arn = aws_iam_role.sagemaker_execution_role.arn\\n\\n  primary_container {\\n    image              = var.ecr_image_uri\\n    model_data_url     = var.model_data_s3_uri\\n    container_hostname = var.container_hostname\\n  }\\n}\\n\\n###############################################################################\\n# ENDPOINT CONFIGURATION\\n###############################################################################\\n\\nresource \"aws_sagemaker_endpoint_configuration\" \"config\" {\\n  name = var.endpoint_config_name\\n\\n  production_variants {\\n    variant_name           = var.production_variant_name\\n    model_name             = aws_sagemaker_model.model.name\\n    initial_instance_count = var.initial_instance_count\\n    instance_type          = var.instance_type\\n  }\\n}\\n\\n###############################################################################\\n# SAGEMAKER ENDPOINT\\n###############################################################################\\n\\nresource \"aws_sagemaker_endpoint\" \"endpoint\" {\\n  name                 = var.endpoint_name\\n  endpoint_config_name = aws_sagemaker_endpoint_configuration.config.name\\n}'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mSaved to /Users/neel/Developer/sagemaker_test2/terraform/main.tf\u001b[0m\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Certainly! Below are the step-by-step instructions to run the Terraform script and deploy your model on AWS SageMaker Endpoint, as well as how to destroy the resources when you're done.\n",
      "\n",
      "### Prerequisites\n",
      "\n",
      "1. **Install Terraform**: Make sure you have Terraform installed on your machine. You can download it from the [Terraform website](https://www.terraform.io/downloads.html).\n",
      "\n",
      "2. **AWS CLI Configuration**: Ensure that your AWS CLI is configured with the necessary credentials. You can configure it using:\n",
      "   ```bash\n",
      "   aws configure\n",
      "   ```\n",
      "   This will prompt you to enter your AWS Access Key, Secret Key, region, and output format.\n",
      "\n",
      "3. **Terraform Initialization**: Ensure you have a working directory for your Terraform files.\n",
      "\n",
      "### Steps to Deploy the Model\n",
      "\n",
      "1. **Create a Directory**: Create a directory for your Terraform project and navigate into it.\n",
      "   ```bash\n",
      "   mkdir sagemaker-deployment\n",
      "   cd sagemaker-deployment\n",
      "   ```\n",
      "\n",
      "2. **Save the Terraform Script**: Save the provided Terraform script into a file named `main.tf` within the directory.\n",
      "\n",
      "3. **Initialize Terraform**: Run the following command to initialize Terraform. This will download the necessary provider plugins.\n",
      "   ```bash\n",
      "   terraform init\n",
      "   ```\n",
      "\n",
      "4. **Plan the Deployment**: Run the following command to see what changes Terraform will make to your AWS environment.\n",
      "   ```bash\n",
      "   terraform plan\n",
      "   ```\n",
      "\n",
      "5. **Apply the Deployment**: Run the following command to apply the changes and deploy the resources. This will prompt you for confirmation before proceeding.\n",
      "   ```bash\n",
      "   terraform apply\n",
      "   ```\n",
      "   Type `yes` when prompted to confirm the deployment.\n",
      "\n",
      "### Destroying the Resources\n",
      "\n",
      "Once you are done with the SageMaker endpoint and want to clean up the resources, you can destroy them using Terraform.\n",
      "\n",
      "1. **Destroy the Resources**: Run the following command to destroy all the resources created by Terraform.\n",
      "   ```bash\n",
      "   terraform destroy\n",
      "   ```\n",
      "   Type `yes` when prompted to confirm the destruction.\n",
      "\n",
      "### Additional Notes\n",
      "\n",
      "- **State File**: Terraform maintains a state file (`terraform.tfstate`) in your working directory to keep track of the resources it manages. Ensure this file is not deleted or modified manually.\n",
      "\n",
      "- **Version Control**: It's a good practice to version control your Terraform files using a system like Git.\n",
      "\n",
      "- **Environment Variables**: If you have multiple AWS profiles, you can specify which profile to use by setting the `AWS_PROFILE` environment variable before running Terraform commands:\n",
      "  ```bash\n",
      "  export AWS_PROFILE=your-profile-name\n",
      "  ```\n",
      "\n",
      "By following these steps, you should be able to deploy and manage your AWS SageMaker resources using Terraform effectively.\n"
     ]
    }
   ],
   "source": [
    "a.orchestrate_terraform_deployment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a42f401",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
