{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules from LangChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from talk_openai import MyOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "import os\n",
    "from utils import *\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gathered information\n",
    "#container name and app name cannot have underscores\n",
    "options = {\n",
    "    \"docker_image\": \"neel26d/iris_model_inference:latest\",\n",
    "    \"docker_container_name\": \"iris_model-inference\",\n",
    "    \"model_name\": 'iris-model',\n",
    "    \"model_port\": 8000,\n",
    "    \"service_port\": 80,\n",
    "    \"number_of_replicas\": 1,\n",
    "    'deployment_template': file_reader('/Users/neel/Developer/deploy_wizard/templates/k8s/deployment.yaml'),\n",
    "    \"service_template\": file_reader('templates/k8s/service.yaml'),\n",
    "    'where_is_cluster': 'minikube',}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "k8s_gen_llm = MyOpenAI(model=\"gpt-4o-mini\")\n",
    "k8_critic = MyOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_kubernetes_yaml(options,feedback=None):\n",
    "    # Generate the Kubernetes YAML configuration\n",
    "    if feedback is None:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert in Kubernetes and Docker. You will be given a set of options to create a Kubernetes YAML configuration for a Docker container. The options will include the Docker image, container name, model name, model port, and service port. You will also be given feedback from a critic. Use this feedback to improve the generated YAML configuration.\"},\n",
    "            {\"role\": \"user\", \n",
    "            \"content\": f'''You are an expert Kubernetes YAML generator.\n",
    "            Generate two YAML files:\n",
    "            1. A Kubernetes Deployment manifest.\n",
    "            2. A Kubernetes Service manifest.\n",
    "\n",
    "            **Deployment Requirements:**\n",
    "            - Application Name: {options['model_name']}\n",
    "            - Container Image: {options['docker_image']}\n",
    "            - Container Port: {options['model_port']}\n",
    "            - Number of Replicas: {options['number_of_replicas']}\n",
    "            - Labels: app: {options['model_name']},\n",
    "            - The container must expose port {options['model_port']}.\n",
    "            - Template to use as reference: {options['deployment_template']}\n",
    "\n",
    "            **Service Requirements:**\n",
    "            - Service Name: {options['model_name']}-service\n",
    "            - Service Type: LoadBalancer\n",
    "            - Expose Port: {options['service_port']} externally.\n",
    "            - Target Port: {options['model_port']} internally.\n",
    "            - Selector should match the Deployment labels.\n",
    "            - Template to use as reference: {options['service_template']}\n",
    "\n",
    "            Format the output with proper YAML syntax.\n",
    "            Provide each manifest separately under clear headings like:\n",
    "            `--- Deployment ---` and `--- Service ---`.\n",
    "            The cluster is in {options['where_is_cluster']}.'''},\n",
    "        ]\n",
    "    else:\n",
    "        messages = feedback\n",
    "    response = k8s_gen_llm.invoke(messages)\n",
    "    return response\n",
    "\n",
    "\n",
    "def evaluate_kubernetes_with_llm(kubernetes_code):\n",
    "    \"\"\"Use an LLM to evaluate the generated Kubernetes YAML and provide feedback.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a Kubernetes expert and YAML code reviewer specializing in deployments and services.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"\"\"\n",
    "        You are reviewing Kubernetes YAML manifests for deploying and exposing an application.\n",
    "\n",
    "        ### Task:\n",
    "        - Analyze the YAMLs for correctness, best practices, and completeness.\n",
    "        - Validate that the Deployment and Service fields are correctly specified (apiVersion, kind, metadata, spec).\n",
    "        - Ensure that labels and selectors match properly between Deployment and Service.\n",
    "        - Check that the container port and Service targetPort are correctly aligned.\n",
    "        - Verify that the Service type (LoadBalancer, NodePort, etc.) is appropriate.\n",
    "        - Confirm the YAML structure is clean, readable, and properly organized.\n",
    "        - Ensure no unnecessary fields or missing mandatory fields are present.\n",
    "        - If the YAMLs are perfect, respond with 'No changes needed.'\n",
    "        - If changes are required, provide a detailed improvement plan.\n",
    "        - Do not introduce new labels, ports, or fields unless absolutely necessary based on the provided YAML.\n",
    "        - Do not ask it to add any stuff for which information is not provided.\n",
    "\n",
    "        ### YAMLs to review:\n",
    "        ```\n",
    "        {kubernetes_code}\n",
    "        ```\n",
    "\n",
    "        ### Expected Output:\n",
    "        - Either 'No changes needed.' OR a structured list of suggestions for improvement.\n",
    "        \"\"\"}\n",
    "    ]\n",
    "\n",
    "    return k8_critic.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actor_critic_k8s(params):\n",
    "    feedback = None  # No feedback initially\n",
    "    # Step 1: Actor generates Terraform script\n",
    "    kubernetes_code = generate_kubernetes_yaml(params, feedback)\n",
    "    for _ in range(2):  # Max iterations: 2\n",
    "        # Step 2: Critic evaluates the Terraform script\n",
    "        critic_feedback = evaluate_kubernetes_with_llm(kubernetes_code)\n",
    "\n",
    "        if \"no changes needed\" in critic_feedback.lower():\n",
    "            print(\"âœ… No changes needed. Returning final Kubernetes YAML.\")\n",
    "            return kubernetes_code  # Early exit if script is sufficient\n",
    "\n",
    "        # Step 3: Actor refines Terraform script using critic's feedback\n",
    "        feedback = [\n",
    "            {\"role\": \"assistant\", \"content\": kubernetes_code},\n",
    "            {\"role\": \"user\", \"content\": f\"Revise based on this feedback:\\n{critic_feedback}\"}\n",
    "        ]\n",
    "\n",
    "        kubernetes_code = generate_kubernetes_yaml(params, feedback)\n",
    "\n",
    "\n",
    "    print(\"ðŸ”„ Max iterations reached. Returning final Kubernetes files.\")\n",
    "    return kubernetes_code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Max iterations reached. Returning final Kubernetes files.\n"
     ]
    }
   ],
   "source": [
    "k8s_files = actor_critic_k8s(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hereâ€™s the revised Kubernetes YAML configuration for the Deployment and Service, incorporating your feedback and suggestions for clarity, completeness, and best practices:\n",
      "\n",
      "```yaml\n",
      "# Deployment for the iris-model application\n",
      "apiVersion: apps/v1\n",
      "kind: Deployment\n",
      "metadata:\n",
      "  name: iris-model\n",
      "  labels:\n",
      "    app: iris-model\n",
      "spec:\n",
      "  replicas: 3  # Increased for high availability; adjust based on expected load\n",
      "  selector:\n",
      "    matchLabels:\n",
      "      app: iris-model\n",
      "  template:\n",
      "    metadata:\n",
      "      labels:\n",
      "        app: iris-model\n",
      "    spec:\n",
      "      containers:\n",
      "      - name: irismodel-inference\n",
      "        image: neel26d/iris_model_inference:latest\n",
      "        ports:\n",
      "        - containerPort: 8000\n",
      "        resources:  # Resource requests and limits for better resource management\n",
      "          requests:\n",
      "            memory: \"64Mi\"  # Adjust based on actual usage\n",
      "            cpu: \"250m\"     # Adjust based on actual usage\n",
      "          limits:\n",
      "            memory: \"128Mi\" # Adjust based on actual usage\n",
      "            cpu: \"500m\"     # Adjust based on actual usage\n",
      "\n",
      "# Service for exposing the iris-model application\n",
      "apiVersion: v1\n",
      "kind: Service\n",
      "metadata:\n",
      "  name: iris-model-service\n",
      "spec:\n",
      "  type: LoadBalancer  # Changed to LoadBalancer for production environments\n",
      "  ports:\n",
      "    - name: http  # Named port for clarity\n",
      "      port: 80\n",
      "      targetPort: 8000\n",
      "  selector:\n",
      "    app: iris-model  # Ensure consistency with Deployment labels\n",
      "\n",
      "# Note: Ensure to monitor resource usage and adjust requests/limits as necessary.\n",
      "# Consider adding a README for deployment and access instructions.\n",
      "```\n",
      "\n",
      "### Summary of Changes:\n",
      "1. **Deployment**:\n",
      "   - Increased the number of replicas to 3 to better handle expected load in a production environment.\n",
      "   - Added comments to clarify the purpose of resource requests and limits, emphasizing the need for monitoring and adjustment based on actual usage.\n",
      "\n",
      "2. **Service**:\n",
      "   - Changed the service type to `LoadBalancer` to provide a more robust solution for exposing the application in production environments.\n",
      "   - Retained the named port `http` for clarity and consistency.\n",
      "\n",
      "3. **Documentation**:\n",
      "   - Added comments to the YAML to explain the purpose of each section and highlight the importance of monitoring resource usage.\n",
      "   - Suggested adding a README for deployment and access instructions, which is essential for sharing the configuration with others.\n",
      "\n",
      "These adjustments enhance the clarity, usability, and robustness of your Kubernetes manifests, making them more suitable for both development and production environments.\n"
     ]
    }
   ],
   "source": [
    "print(k8s_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "import os\n",
    "\n",
    "# Define the saving tool\n",
    "@tool\n",
    "def save_file_tool(file_path: str, content: str) -> str:\n",
    "    \"\"\"Save content to a file at the given path.\"\"\"\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(content)\n",
    "    return f\"Saved to {file_path}\"\n",
    "\n",
    "# Define the function\n",
    "def extract_and_save_k8s_files(k8s_text: str, save_dir: str = './k8s_files'):\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "    tools = [save_file_tool]\n",
    "\n",
    "    agent = initialize_agent(\n",
    "        tools=tools,\n",
    "        llm=llm,\n",
    "        agent=AgentType.OPENAI_FUNCTIONS,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    response = agent.invoke(\n",
    "        f\"\"\"\n",
    "        1. Extract all Kubernetes YAML code blocks from the given text.\n",
    "        2. For each extracted YAML:\n",
    "            - If it is a Deployment, save it as '{save_dir}/deployment.yaml'.\n",
    "            - If it is a Service, save it as '{save_dir}/service.yaml'.\n",
    "            - Otherwise, save it as '{save_dir}/other.yaml'.\n",
    "        3. Use the `save_file_tool` tool to save the content.\n",
    "        4. Do not output anything else except confirming tool usage.\n",
    "\n",
    "        ### Text:\n",
    "        {k8s_text}\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `save_file_tool` with `{'file_path': './k8s_files/deployment.yaml', 'content': '# Deployment for the iris-model application\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: iris-model\\n  labels:\\n    app: iris-model\\nspec:\\n  replicas: 3  # Increased for high availability; adjust based on expected load\\n  selector:\\n    matchLabels:\\n      app: iris-model\\n  template:\\n    metadata:\\n      labels:\\n        app: iris-model\\n    spec:\\n      containers:\\n      - name: irismodel-inference\\n        image: neel26d/iris_model_inference:latest\\n        ports:\\n        - containerPort: 8000\\n        resources:  # Resource requests and limits for better resource management\\n          requests:\\n            memory: \"64Mi\"  # Adjust based on actual usage\\n            cpu: \"250m\"     # Adjust based on actual usage\\n          limits:\\n            memory: \"128Mi\" # Adjust based on actual usage\\n            cpu: \"500m\"     # Adjust based on actual usage\\n'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mSaved to ./k8s_files/deployment.yaml\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `save_file_tool` with `{'file_path': './k8s_files/service.yaml', 'content': '# Service for exposing the iris-model application\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: iris-model-service\\nspec:\\n  type: LoadBalancer  # Changed to LoadBalancer for production environments\\n  ports:\\n    - name: http  # Named port for clarity\\n      port: 80\\n      targetPort: 8000\\n  selector:\\n    app: iris-model  # Ensure consistency with Deployment labels\\n'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mSaved to ./k8s_files/service.yaml\u001b[0m\u001b[32;1m\u001b[1;3mThe YAML files have been saved successfully.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response= extract_and_save_k8s_files(k8s_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_validator = MyOpenAI(model=\"gpt-4o\")\n",
    "def k8s_deployment_validator(saved_files_dir: str = './k8s_files'):\n",
    "    files = os.listdir(saved_files_dir)\n",
    "    contents = ''\n",
    "    for file in files:\n",
    "        with open(os.path.join(saved_files_dir, file), 'r') as f:\n",
    "            contents += file\n",
    "            contents += f.read()\n",
    "    msg = f'''\n",
    "    The user has deployed a buch of kubernetes stuff \\n {contents} on {options['where_is_cluster']}. \\n\n",
    "    Please help the user validate if everything is working as expected. And provide instruction on how to test the service from outside the cluster. \\n'''\n",
    "    response = llm_validator.invoke(msg)\n",
    "    print(response)\n",
    "    while True:\n",
    "        msg = input(\"Enter your response, if everything is working say gg: \")\n",
    "        if msg == 'gg':\n",
    "            break\n",
    "        print(\"User: \",msg)\n",
    "        response = llm_validator.invoke(msg)\n",
    "        print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To validate that your Kubernetes deployment and service are working as expected on Minikube, and to test the service from outside the cluster, follow these steps:\n",
      "\n",
      "### Step 1: Verify Deployment and Service\n",
      "\n",
      "1. **Check the Deployment:**\n",
      "   - Ensure that the deployment is up and running with the desired number of replicas:\n",
      "     ```bash\n",
      "     kubectl get deployments\n",
      "     ```\n",
      "   - You should see the `iris-model` deployment listed with 3 replicas.\n",
      "\n",
      "2. **Check the Pods:**\n",
      "   - Verify that the pods are running:\n",
      "     ```bash\n",
      "     kubectl get pods\n",
      "     ```\n",
      "   - Ensure that all pods associated with the `iris-model` deployment are in the `Running` state.\n",
      "\n",
      "3. **Check the Service:**\n",
      "   - Verify that the service is created and has an external IP assigned:\n",
      "     ```bash\n",
      "     kubectl get services\n",
      "     ```\n",
      "   - You should see the `iris-model-service` with a `LoadBalancer` type. The `EXTERNAL-IP` column might show `<pending>` initially, but with Minikube, you will use `minikube tunnel` to expose the service.\n",
      "\n",
      "### Step 2: Expose the Service with Minikube\n",
      "\n",
      "1. **Run Minikube Tunnel:**\n",
      "   - Open a new terminal window and run:\n",
      "     ```bash\n",
      "     minikube tunnel\n",
      "     ```\n",
      "   - This command will require elevated privileges and will expose the LoadBalancer service to your local network.\n",
      "\n",
      "2. **Check for External IP:**\n",
      "   - After running `minikube tunnel`, check the service again to see if it has an external IP:\n",
      "     ```bash\n",
      "     kubectl get services\n",
      "     ```\n",
      "   - The `EXTERNAL-IP` should now show an IP address.\n",
      "\n",
      "### Step 3: Test the Service from Outside the Cluster\n",
      "\n",
      "1. **Access the Service:**\n",
      "   - Use the external IP and port 80 to access the service from your browser or a tool like `curl`:\n",
      "     ```bash\n",
      "     curl -X POST -H \"Content-Type: application/json\" -d '{ \"sepal_length\": 5.1, \"sepal_width\": 3.5, \"petal_length\": 1.4, \"petal_width\": 0.2 }' http://<external-ip>/predict\n",
      "     ```\n",
      "   - Replace `<external-ip>` with the IP address from the `kubectl get services` output.\n",
      "\n",
      "### Troubleshooting\n",
      "\n",
      "- **Pods Not Running:**\n",
      "  - If the pods are not running, check the pod logs for errors:\n",
      "    ```bash\n",
      "    kubectl logs <pod-name>\n",
      "    ```\n",
      "  - Replace `<pod-name>` with the name of your pod.\n",
      "\n",
      "- **Service Not Accessible:**\n",
      "  - Ensure that `minikube tunnel` is running and that there are no network restrictions on your local machine.\n",
      "\n",
      "- **Application Errors:**\n",
      "  - Check the application logs to ensure it is correctly handling requests at the `/predict` endpoint.\n",
      "\n",
      "By following these steps, you should be able to validate and test your Kubernetes deployment and service on Minikube. If you encounter any issues, ensure that your Minikube setup is correct and that there are no network restrictions preventing access to the service.\n"
     ]
    }
   ],
   "source": [
    "k8s_deployment_validator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
